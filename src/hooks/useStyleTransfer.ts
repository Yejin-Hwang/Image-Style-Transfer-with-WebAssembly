import { useState, useCallback } from 'react'
import { type PreprocessedImage, ImagePreprocessor } from '@/utils/imagePreprocessing'
import { type OnnxModelConfig } from '@/utils/onnxModels'
import * as ort from 'onnxruntime-web'

export interface StyleTransferResult {
  originalImage: string
  styledImage: string | null
  processingTime: number | null
  error: string | null
  // ONNX ì—°ê²° ìƒíƒœ ë° ì²˜ë¦¬ ë°©ì‹
  onnxStatus: 'connected' | 'failed' | 'not_attempted'
  processingMethod: 'onnx' | 'simulation' | 'none'
  statusMessage: string
  // Tensor data for debugging and reprocessing
  inputTensor?: {
    data: Float32Array
    dims: readonly number[]
    dataRange: { min: number; max: number }
  }
  outputTensor?: {
    data: Float32Array
    dims: readonly number[]
    dataRange: { min: number; max: number }
  }

}
interface UseStyleTransferReturn {
  result: StyleTransferResult | null
  isTransferring: boolean
  transferStyle: (preprocessedImage: PreprocessedImage, modelConfig: OnnxModelConfig, originalImageUrl?: string) => Promise<void>
  clearResult: () => void
}

// ONNX ëª¨ë¸ ìºì‹œ
const modelCache = new Map<string, ort.InferenceSession>()

// ì•ˆì „í•œ ë°°ì—´ ìµœì†Œ/ìµœëŒ€ê°’ ê³„ì‚° í•¨ìˆ˜
function getArrayMinMax(array: Float32Array): { min: number; max: number } {
  if (array.length === 0) return { min: 0, max: 0 }
  
  let min = array[0]
  let max = array[0]
  
  for (let i = 1; i < array.length; i++) {
    if (array[i] < min) min = array[i]
    if (array[i] > max) max = array[i]
  }
  
  return { min, max }
}

export function useStyleTransfer(): UseStyleTransferReturn {
  const [result, setResult] = useState<StyleTransferResult | null>(null)
  const [isTransferring, setIsTransferring] = useState(false)

  const transferStyle = useCallback(async (
    preprocessedImage: PreprocessedImage,
    modelConfig: OnnxModelConfig,
    originalImageUrl?: string
  ) => {
    setIsTransferring(true)
    setResult(null)

    try {
      const startTime = performance.now()
      
      let styledImage: string
      let inputTensor: ort.Tensor | undefined
      let outputTensor: ort.Tensor | undefined

      
      try {
        // ì‹¤ì œ ONNX ëª¨ë¸ ì¶”ë¡  ì‹œë„
        console.log('Attempting ONNX model inference...')
        const inferenceResult = await runOnnxInference(preprocessedImage, modelConfig)
        styledImage = inferenceResult.styledImage
        inputTensor = inferenceResult.inputTensor
        outputTensor = inferenceResult.outputTensor
        console.log('ONNX inference successful')
      } catch (onnxError) {
        console.warn('ONNX inference failed, falling back to simulation:', onnxError)
        
        // ONNX ì‹¤íŒ¨ ì‹œ ì‹œë®¬ë ˆì´ì…˜ìœ¼ë¡œ fallback
        styledImage = await simulateStyleTransfer(preprocessedImage, modelConfig)
        console.log('Fallback simulation completed')
      }
      
      const processingTime = performance.now() - startTime
      
      // ONNX ì„±ê³µ ì—¬ë¶€ì— ë”°ë¼ ìƒíƒœ ì„¤ì •
      const onnxStatus = inputTensor ? 'connected' : 'failed'
      const processingMethod = inputTensor ? 'onnx' : 'simulation'
      const statusMessage = inputTensor 
        ? 'ONNX ëª¨ë¸ ì¶”ë¡ ì´ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.'
        : 'ONNX ëª¨ë¸ ì—°ê²°ì— ì‹¤íŒ¨í•˜ì—¬ ì‹œë®¬ë ˆì´ì…˜ ê²°ê³¼ë¥¼ í‘œì‹œí•©ë‹ˆë‹¤.'
      
      setResult({
        originalImage: originalImageUrl || '',
        styledImage,
        processingTime,
        error: null,
        onnxStatus,
        processingMethod,
        statusMessage,
        inputTensor: inputTensor ? {
          data: inputTensor.data as Float32Array,
          dims: inputTensor.dims,
          dataRange: getArrayMinMax(inputTensor.data as Float32Array)
        } : undefined,
        outputTensor: outputTensor ? {
          data: outputTensor.data as Float32Array,
          dims: outputTensor.dims,
          dataRange: getArrayMinMax(outputTensor.data as Float32Array)
        } : undefined
      })
    } catch (error) {
      const errorMessage = error instanceof Error ? error.message : 'Style transfer failed'
      console.error('Style transfer error:', error)
      setResult({
        originalImage: '',
        styledImage: null,
        processingTime: null,
        error: errorMessage,
        onnxStatus: 'failed',
        processingMethod: 'none',
        statusMessage: `ìŠ¤íƒ€ì¼ ë³€í™˜ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: ${errorMessage}`
      })
    } finally {
      setIsTransferring(false)
    }
  }, [])

  const clearResult = useCallback(() => {
    setResult(null)
  }, [])

  return {
    result,
    isTransferring,
    transferStyle,
    clearResult
  }
}

// ì‹¤ì œ ONNX ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
async function runOnnxInference(
  preprocessedImage: PreprocessedImage,
  modelConfig: OnnxModelConfig
): Promise<{
  styledImage: string
  inputTensor: ort.Tensor
  outputTensor: ort.Tensor
}> {
  try {
    console.log('Starting ONNX inference...')
    console.log('Model config:', modelConfig)
    console.log('Preprocessed image:', {
      width: preprocessedImage.width,
      height: preprocessedImage.height,
      dataLength: preprocessedImage.data.length,
      dataType: preprocessedImage.data.constructor.name,
      tensorShape: preprocessedImage.tensorShape,
      normalizationType: preprocessedImage.normalizationType,
      dataRange: preprocessedImage.dataRange
    })
    
    // 1. ëª¨ë¸ ë¡œë“œ (ìºì‹œëœ ê²½ìš° ì¬ì‚¬ìš©)
    const session = await loadModel(modelConfig.filename)
    
    // 2. ëª¨ë¸ ë©”íƒ€ë°ì´í„° í™•ì¸
    console.log('Model input names:', session.inputNames)
    console.log('Model output names:', session.outputNames)
    
    // 3. ì…ë ¥ í…ì„œ ê²€ì¦ ë° ì¤€ë¹„
    console.log('Validating input tensor...')
    const inputTensor = prepareInputTensor(preprocessedImage, modelConfig)
    
    // 4. í…ì„œ ë°ì´í„° ê²€ì¦
    const tensorData = inputTensor.data as Float32Array
    const tensorDims = inputTensor.dims
    
    console.log('Input tensor validation:', {
      shape: tensorDims,
      dataType: inputTensor.type,
      dataLength: tensorData.length,
      dataRange: getArrayMinMax(tensorData),
      expectedShape: modelConfig.inputShape
    })
    
    // Validate tensor dimensions match model expectations
    if (tensorDims.length !== modelConfig.inputShape.length) {
      throw new Error(`Tensor dimension mismatch. Expected ${modelConfig.inputShape.length}D, got ${tensorDims.length}D`)
    }
    
    for (let i = 0; i < tensorDims.length; i++) {
      if (tensorDims[i] !== modelConfig.inputShape[i]) {
        throw new Error(`Tensor dimension ${i} mismatch. Expected ${modelConfig.inputShape[i]}, got ${tensorDims[i]}`)
      }
    }
    console.log('Input tensor details:', {
      dims: inputTensor.dims,
      type: inputTensor.type,
      dataLength: inputTensor.data.length,
      shape: inputTensor.dims.join('x')
    })
    
    // ëª¨ë¸ì´ ê¸°ëŒ€í•˜ëŠ” ì…ë ¥ê³¼ ë¹„êµ
    console.log('Expected input shape from config:', modelConfig.inputShape)
    console.log('Actual tensor shape:', inputTensor.dims)
    
    // 4. ëª¨ë¸ ì¶”ë¡  ì‹¤í–‰
    console.log('Running model inference...')
    const outputs = await session.run({
      [session.inputNames[0]]: inputTensor
    })
    console.log('Model outputs:', outputs)
    
    // 5. ì¶œë ¥ ë°ì´í„°ë¥¼ ì´ë¯¸ì§€ë¡œ ë³€í™˜
    const styledImage = await convertOutputToImage(outputs, modelConfig)
    console.log('Style transfer completed successfully')
    
    // Get the output tensor for postprocessing
    const outputKey = Object.keys(outputs)[0]
    const outputTensor = outputs[outputKey] as ort.Tensor
    
    console.log('Style transfer completed successfully')
    
    return {
      styledImage,
      inputTensor,
      outputTensor
    }
  } catch (error) {
    console.error('ONNX inference failed:', error)
    throw new Error(`Model inference failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
  }
}

// ONNX ëª¨ë¸ ë¡œë“œ
async function loadModel(filename: string): Promise<ort.InferenceSession> {
  // ìºì‹œëœ ëª¨ë¸ì´ ìˆìœ¼ë©´ ë°˜í™˜
  if (modelCache.has(filename)) {
    console.log(`Using cached model: ${filename}`)
    return modelCache.get(filename)!
  }
  
  try {
    console.log(`Loading ONNX model: ${filename}`)
    
    // ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
    const modelPath = `/models/${filename}`
    console.log(`Model path: ${modelPath}`)
    
    // ëª¨ë¸ ë¡œë“œ
    console.log(`Loading model: ${filename}...`)
    const session = await ort.InferenceSession.create(modelPath, {
      executionProviders: ['wasm'],
      graphOptimizationLevel: 'disabled', // ê·¸ë˜í”„ ìµœì í™” ì™„ì „ ë¹„í™œì„±í™”í•˜ì—¬ ê²½ê³  ì œê±°
      enableCpuMemArena: false, // ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™” ë¹„í™œì„±í™”
      enableMemPattern: false, // ë©”ëª¨ë¦¬ íŒ¨í„´ ìµœì í™” ë¹„í™œì„±í™”
      logSeverityLevel: 3 // ì—ëŸ¬ë§Œ í‘œì‹œ (0: verbose, 1: info, 2: warning, 3: error, 4: fatal)
    })
    
    // ìºì‹œì— ì €ì¥
    modelCache.set(filename, session)
    
    console.log(`âœ… Model loaded successfully: ${filename}`)
    console.log(`ğŸ“Š Model details:`, {
      inputNames: session.inputNames,
      outputNames: session.outputNames,
      executionProviders: session.executionProviders
    })
    return session
  } catch (error) {
    console.error(`âŒ Failed to load model ${filename}:`, error)
    
    // ONNX Runtime ê²½ê³ ëŠ” ì •ìƒ ì‘ë™ì— ì˜í–¥ ì—†ìŒì„ ëª…ì‹œ
    if (error instanceof Error && error.message.includes('Initializer')) {
      console.info(`â„¹ï¸ Note: ONNX Runtime warnings about initializers are normal and don't affect functionality`)
    }
    
    throw new Error(`Failed to load model: ${error instanceof Error ? error.message : 'Unknown error'}`)
  }
}

// ì…ë ¥ í…ì„œ ì¤€ë¹„ (NHWC í˜•ì‹ìœ¼ë¡œ ìˆ˜ì •)
function prepareInputTensor(
  preprocessedImage: PreprocessedImage,
  modelConfig: OnnxModelConfig
): ort.Tensor {
  // Use the expected model input shape instead of the preprocessed image shape
  const [batch, targetHeight, targetWidth, channels] = modelConfig.inputShape
  
  console.log('Preparing input tensor with expected model shape:', [batch, targetHeight, targetWidth, channels])
  console.log('Preprocessed image dimensions:', {
    width: preprocessedImage.width,
    height: preprocessedImage.height,
    dataLength: preprocessedImage.data.length,
    normalizationType: preprocessedImage.normalizationType,
    dataRange: preprocessedImage.dataRange
  })
  
  // Validate tensor compatibility
  const validation = ImagePreprocessor.validatePreprocessedImage(preprocessedImage, modelConfig)
  if (!validation.isValid) {
    throw new Error(`Tensor validation failed: ${validation.error}`)
  }
  
  if (validation.warnings) {
    console.warn('Tensor validation warnings:', validation.warnings)
  }
  
  // Check if data length matches expected tensor size
  const expectedDataLength = batch * targetHeight * targetWidth * channels
  if (preprocessedImage.data.length !== expectedDataLength) {
    console.warn(`Data length mismatch: expected ${expectedDataLength}, got ${preprocessedImage.data.length}`)
    console.warn('This may indicate the image was not properly resized to target dimensions')
    
    // For now, we'll use the data as-is, but this should be fixed in preprocessing
    // The issue is likely that maintainAspectRatio is true, causing non-square dimensions
  }
  
  // Use the preprocessed data directly (already in correct format)
  const inputData = preprocessedImage.data
  
  // Create tensor with the expected model input shape
  const tensor = new ort.Tensor('float32', inputData, [batch, targetHeight, targetWidth, channels])
  
  console.log('Input tensor prepared successfully')
  console.log('Final tensor shape:', [batch, targetHeight, targetWidth, channels])
  console.log('Tensor data range:', preprocessedImage.dataRange)
  console.log('Normalization type:', preprocessedImage.normalizationType)
  
  return tensor
}

// ëª¨ë¸ ì¶œë ¥ì„ ì´ë¯¸ì§€ë¡œ ë³€í™˜
async function convertOutputToImage(
  outputs: ort.InferenceSession.OnnxValueMapType,
  modelConfig: OnnxModelConfig
): Promise<string> {
  try {
    console.log('Converting model output to image...')
    
    // ì¶œë ¥ í…ì„œ ê°€ì ¸ì˜¤ê¸° (ì²« ë²ˆì§¸ ì¶œë ¥ ì‚¬ìš©)
    const outputKey = Object.keys(outputs)[0]
    const outputTensor = outputs[outputKey] as ort.Tensor
    
    if (!outputTensor) {
      throw new Error('No output tensor found')
    }
    
    console.log('Output tensor:', {
      dims: outputTensor.dims,
      type: outputTensor.type,
      dataLength: outputTensor.data.length
    })
    
    // í…ì„œ ë°ì´í„° ì¶”ì¶œ
    const outputData = outputTensor.data as Float32Array
    const [, height, width, channels] = outputTensor.dims
    
    // Canvas ìƒì„±
    const canvas = document.createElement('canvas')
    canvas.width = width
    canvas.height = height
    
    const ctx = canvas.getContext('2d')
    if (!ctx) {
      throw new Error('Failed to get canvas context')
    }
    
    // ImageData ìƒì„±
    const imageData = ctx.createImageData(width, height)
    const data = imageData.data
    
    // Float32Arrayë¥¼ Uint8ClampedArrayë¡œ ë³€í™˜
    for (let i = 0; i < height * width; i++) {
      const pixelIndex = i * 4
      const dataIndex = i * channels
      
      // RGB ê°’ ì¶”ì¶œ (ëª¨ë¸ë³„ ì •ê·œí™” ë²”ìœ„ì— ë”°ë¼ ë³€í™˜)
      let r, g, b
      
      // Determine normalization type based on model config
      const isAnimeGAN = modelConfig.mean[0] === 0.5 && modelConfig.std[0] === 0.5
      const isImageNet = modelConfig.mean[0] === 0.485 && modelConfig.std[0] === 0.229
      
      if (isAnimeGAN) {
        // AnimeGAN ëª¨ë¸: -1~1 ë²”ìœ„ë¥¼ 0-255ë¡œ ë³€í™˜
        r = Math.max(0, Math.min(255, Math.round(((outputData[dataIndex] + 1) / 2) * 255)))
        g = Math.max(0, Math.min(255, Math.round(((outputData[dataIndex + 1] + 1) / 2) * 255)))
        b = Math.max(0, Math.min(255, Math.round(((outputData[dataIndex + 2] + 1) / 2) * 255)))
      } else if (isImageNet) {
        // ImageNet ì •ê·œí™” ëª¨ë¸: 0~1 ë²”ìœ„ë¥¼ 0-255ë¡œ ë³€í™˜
        r = Math.max(0, Math.min(255, Math.round(outputData[dataIndex] * 255)))
        g = Math.max(0, Math.min(255, Math.round(outputData[dataIndex + 1] * 255)))
        b = Math.max(0, Math.min(255, Math.round(outputData[dataIndex + 2] * 255)))
      } else {
        // Custom normalization: assume output is in 0-1 range
        r = Math.max(0, Math.min(255, Math.round(outputData[dataIndex] * 255)))
        g = Math.max(0, Math.min(255, Math.round(outputData[dataIndex + 1] * 255)))
        b = Math.max(0, Math.min(255, Math.round(outputData[dataIndex + 2] * 255)))
      }
      
      // í”½ì…€ ë°ì´í„° ì„¤ì •
      data[pixelIndex] = r
      data[pixelIndex + 1] = g
      data[pixelIndex + 2] = b
      data[pixelIndex + 3] = 255 // Alpha
    }
    
    // Canvasì— ì´ë¯¸ì§€ ë°ì´í„° ì ìš©
    ctx.putImageData(imageData, 0, 0)
    
    // Canvasë¥¼ data URLë¡œ ë³€í™˜
    const dataUrl = canvas.toDataURL('image/png')
    console.log('Image conversion completed')
    
    return dataUrl
  } catch (error) {
    console.error('Failed to convert output to image:', error)
    throw new Error(`Output conversion failed: ${error instanceof Error ? error.message : 'Unknown error'}`)
  }
}

// ì‹œë®¬ë ˆì´ì…˜ëœ ìŠ¤íƒ€ì¼ ì „ì†¡ í•¨ìˆ˜ (fallbackìš©)
async function simulateStyleTransfer(
  preprocessedImage: PreprocessedImage,
  modelConfig: OnnxModelConfig
): Promise<string> {
  console.log('Running simulation style transfer...')
  
  // Canvasë¥¼ ì‚¬ìš©í•˜ì—¬ ìŠ¤íƒ€ì¼ì´ ì ìš©ëœ ì´ë¯¸ì§€ ì‹œë®¬ë ˆì´ì…˜
  const canvas = document.createElement('canvas')
  canvas.width = preprocessedImage.width
  canvas.height = preprocessedImage.height
  
  const ctx = canvas.getContext('2d')
  if (!ctx) {
    throw new Error('Failed to get canvas context')
  }

  // ì›ë³¸ ì´ë¯¸ì§€ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ê·¸ë¦¬ê¸°
  const imageData = ctx.createImageData(preprocessedImage.width, preprocessedImage.height)
  const data = imageData.data
  
  // Float32Arrayë¥¼ Uint8ClampedArrayë¡œ ë³€í™˜ (ì‹œë®¬ë ˆì´ì…˜ìš©)
  for (let i = 0; i < preprocessedImage.data.length; i += 3) {
    const pixelIndex = (i / 3) * 4
    const x = (i / 3) % preprocessedImage.width
    const y = Math.floor((i / 3) / preprocessedImage.width)
    
    // ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ìƒ‰ìƒ ë³€í™˜ ì‹œë®¬ë ˆì´ì…˜
    let r = Math.round(preprocessedImage.data[i] * 255)
    let g = Math.round(preprocessedImage.data[i + 1] * 255)
    let b = Math.round(preprocessedImage.data[i + 2] * 255)
    
    // ìŠ¤íƒ€ì¼ë³„ ìƒ‰ìƒ í•„í„° ì ìš©
    if (modelConfig.name.includes('Anime')) {
      // Anime ìŠ¤íƒ€ì¼ì€ ì‹œë®¬ë ˆì´ì…˜ fallbackì„ ì§€ì›í•˜ì§€ ì•ŠìŒ
      // ë°˜ë“œì‹œ ONNX ì¶”ë¡ ì„ í†µí•´ì„œë§Œ ì ìš©ë˜ì–´ì•¼ í•¨
      throw new Error(
        'Anime ìŠ¤íƒ€ì¼ì€ public/models í´ë”ì˜ Anime ONNX ëª¨ë¸ì„ ì§ì ‘ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤. ONNX ì¶”ë¡ ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.'
      )
    } else if (modelConfig.name.includes('Picasso')) {
      // í”¼ì¹´ì†Œ ìŠ¤íƒ€ì¼: ëŒ€ë¹„ ê°•í™”
      r = r > 128 ? Math.min(255, r * 1.4) : Math.max(0, r * 0.6)
      g = g > 128 ? Math.min(255, g * 1.4) : Math.max(0, g * 0.6)
      b = b > 128 ? Math.min(255, b * 1.4) : Math.max(0, b * 0.6)
    } else if (modelConfig.name.includes('Van Gogh')) {
      // ë°˜ ê³ í ìŠ¤íƒ€ì¼: ë‘êº¼ìš´ ë¶“ë†€ë¦¼ê³¼ ì†Œìš©ëŒì´ì¹˜ëŠ” íŒ¨í„´ ì‹œë®¬ë ˆì´ì…˜
      // ìœ„ì¹˜ ê¸°ë°˜ íŒŒë™ íš¨ê³¼ (ë¶“ë†€ë¦¼ ì‹œë®¬ë ˆì´ì…˜)
      const waveX = Math.sin(x * 0.1) * 0.2
      const waveY = Math.cos(y * 0.08) * 0.3
      const intensity = (waveX + waveY) * 0.5 + 1.0
      
      // ë°˜ ê³ í íŠ¹ìœ ì˜ ë”°ëœ»í•œ ìƒ‰ê° (ë…¸ë€ìƒ‰, ì£¼í™©ìƒ‰, ë¹¨ê°„ìƒ‰ ê°•ì¡°)
      r = Math.min(255, r * (1.5 + intensity * 0.3))
      g = Math.min(255, g * (1.4 + intensity * 0.2))
      b = Math.max(0, b * (0.6 + intensity * 0.1))
      
      // ìƒ‰ìƒ ëŒ€ë¹„ ê°•í™”
      if (r > 128) r = Math.min(255, r + 25)
      if (g > 128) g = Math.min(255, g + 20)
      b = Math.max(0, b - 40)
      
      // ë°˜ ê³ í ìŠ¤íƒ€ì¼ì˜ ìƒ‰ìƒ ì¡°í™”
      const brightness = (r + g + b) / 3
      if (brightness > 150) {
        // ë°ì€ ë¶€ë¶„ì€ ë” ë”°ëœ»í•˜ê²Œ
        r = Math.min(255, r + 15)
        g = Math.min(255, g + 10)
      } else {
        // ì–´ë‘ìš´ ë¶€ë¶„ì€ ë” ê¹Šê²Œ
        r = Math.max(0, r - 20)
        g = Math.max(0, g - 15)
        b = Math.max(0, b - 30)
      }
    } else if (modelConfig.name.includes('Cyberpunk')) {
      // ì‚¬ì´ë²„í‘í¬ ìŠ¤íƒ€ì¼: ë„¤ì˜¨ íš¨ê³¼
      r = Math.min(255, r * 1.5)
      g = Math.max(0, g * 0.7)
      b = Math.min(255, b * 1.8)
    }
    
    data[pixelIndex] = Math.max(0, Math.min(255, r))
    data[pixelIndex + 1] = Math.max(0, Math.min(255, g))
    data[pixelIndex + 2] = Math.max(0, Math.min(255, b))
    data[pixelIndex + 3] = 255 // Alpha
  }
  
  ctx.putImageData(imageData, 0, 0)
  
  // ìŠ¤íƒ€ì¼ë³„ ì¶”ê°€ íš¨ê³¼ ì ìš©
  if (modelConfig.name.includes('Van Gogh')) {
    applyVanGoghBrushStrokes(ctx, preprocessedImage.width, preprocessedImage.height)
  } else if (modelConfig.name.includes('Anime')) {
    applyAnimeEffects(ctx, preprocessedImage.width, preprocessedImage.height)
  }
  
  // Canvasë¥¼ data URLë¡œ ë³€í™˜
  return canvas.toDataURL('image/png')
}

// ë°˜ ê³ í ìŠ¤íƒ€ì¼ì˜ ë¶“ë†€ë¦¼ íš¨ê³¼ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
function applyVanGoghBrushStrokes(
  ctx: CanvasRenderingContext2D,
  width: number,
  height: number
) {
  // ë¶“ë†€ë¦¼ íš¨ê³¼ë¥¼ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´
  ctx.globalCompositeOperation = 'overlay'
  ctx.globalAlpha = 0.3
  
  // ì—¬ëŸ¬ ë°©í–¥ì˜ ë¶“ë†€ë¦¼ ì„  ê·¸ë¦¬ê¸°
  for (let i = 0; i < 50; i++) {
    const x = Math.random() * width
    const y = Math.random() * height
    const length = 20 + Math.random() * 40
    const angle = Math.random() * Math.PI * 2
    
    ctx.strokeStyle = `hsl(${45 + Math.random() * 30}, 70%, 60%)` // ë…¸ë€ìƒ‰-ì£¼í™©ìƒ‰ ê³„ì—´
    ctx.lineWidth = 2 + Math.random() * 3
    ctx.lineCap = 'round'
    
    ctx.beginPath()
    ctx.moveTo(x, y)
    ctx.lineTo(
      x + Math.cos(angle) * length,
      y + Math.sin(angle) * length
    )
    ctx.stroke()
  }
  
  // ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›
  ctx.globalCompositeOperation = 'source-over'
  ctx.globalAlpha = 1.0
}

// Anime ìŠ¤íƒ€ì¼ íš¨ê³¼ë¥¼ ì¶”ê°€í•˜ëŠ” í•¨ìˆ˜
function applyAnimeEffects(
  ctx: CanvasRenderingContext2D,
  width: number,
  height: number
) {
  // ë¶€ë“œëŸ¬ìš´ ìƒ‰ìƒ ê·¸ë¼ë°ì´ì…˜ íš¨ê³¼
  ctx.globalCompositeOperation = 'soft-light'
  ctx.globalAlpha = 0.3
  
  // ì—¬ëŸ¬ ë°©í–¥ì˜ ë¶€ë“œëŸ¬ìš´ ê·¸ë¼ë°ì´ì…˜
  for (let i = 0; i < 3; i++) {
    const gradient = ctx.createLinearGradient(
      Math.random() * width,
      Math.random() * height,
      Math.random() * width,
      Math.random() * height
    )
    
    // Ghibli íŠ¹ìœ ì˜ ë”°ëœ»í•œ ìƒ‰ìƒ
    gradient.addColorStop(0, 'rgba(255, 255, 200, 0.1)') // ì—°í•œ ë…¸ë€ìƒ‰
    gradient.addColorStop(0.5, 'rgba(200, 255, 200, 0.1)') // ì—°í•œ ì´ˆë¡ìƒ‰
    gradient.addColorStop(1, 'rgba(255, 200, 200, 0.1)') // ì—°í•œ ë¹¨ê°„ìƒ‰
    
    ctx.fillStyle = gradient
    ctx.fillRect(0, 0, width, height)
  }
  
  // ì„ ëª…í•œ ìœ¤ê³½ì„  íš¨ê³¼
  ctx.globalCompositeOperation = 'overlay'
  ctx.globalAlpha = 0.2
  
  // ì´ë¯¸ì§€ ê°€ì¥ìë¦¬ì— ë¶€ë“œëŸ¬ìš´ í…Œë‘ë¦¬ ì¶”ê°€
  ctx.strokeStyle = 'rgba(100, 150, 100, 0.3)'
  ctx.lineWidth = 2
  ctx.strokeRect(2, 2, width - 4, height - 4)
  
  // ì›ë˜ ì„¤ì •ìœ¼ë¡œ ë³µì›
  ctx.globalCompositeOperation = 'source-over'
  ctx.globalAlpha = 1.0
}

// ëª¨ë¸ ìºì‹œ ì •ë¦¬ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
export function clearModelCache(): void {
  for (const [filename, session] of modelCache.entries()) {
    try {
      session.release()
      console.log(`Released model: ${filename}`)
    } catch (error) {
      console.error(`Failed to release model ${filename}:`, error)
    }
  }
  modelCache.clear()
}
